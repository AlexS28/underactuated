<!DOCTYPE html>

<html xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
 xsi:schemaLocation="http://www.w3.org/1999/xhtml HTMLBook/schema/htmlbook.xsd"
 xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Underactuated Robotics</title>
    <meta name="Underactuated Robotics" content="text/html; charset=utf-8;" />

    <script type="text/javascript">
      function getParameterByName(name) {
        name = name.replace(/[\[]/, "\\[").replace(/[\]]/, "\\]");
        var regex = new RegExp("[\\?&]" + name + "=([^&#]*)"),
          results = regex.exec(location.search);
        return results == null ? "" : decodeURIComponent(results[1].replace(/\+/g, " "));
      }

      function revealChapters() {
        var chapter = getParameterByName("chapter");
        if (chapter) {
          chapter = Number(chapter);
          var chapters = document.getElementsByClassName("chapter");
          var i;
          //document.getElementById("debug_output").innerHTML="displaying only chapter " + chapter + " of " + chapters.length;
          for (i = 0; i < chapters.length; i++) {
            if ((i+1) != chapter) {
              chapters[i].style.display = "none";
            } else {
//              document.getElementById("debug_output").innerHTML="got here";
              chapters[i].style.counterReset = "chapter " + i;
            }
          }
        }

        var appendix = getParameterByName("appendix");
        if (appendix) {
          var app = document.getElementsByTagName("appendix");
          var chapters = document.getElementsByClassName("chapter");
          var i;
          for (i = 0; i < chapters.length; i++) {
            chapters[i].style.display = "none";
          }
          var chapters = app[0].getElementsByClassName("chapter");
          i = Number(appendix)-1;
          chapters[i].style.display = "inline";
          chapters[i].style.counterReset = "chapter " + i;
        }
      }
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        TeX: { equationNumbers: {autoNumber: "AMS"} },
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
      src="MathJax/MathJax.js?config=TeX-AMS_HTML">
    </script>

    <link rel="stylesheet" href="highlight/styles/default.css">
    <script src="highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script type="text/javascript">
      function bibtex(){
        var elibTags=document.getElementsByTagName('elib');
        for(i=0;i<elibTags.length;i++)
        {
          var c = elibTags[i].innerHTML;
          elibTags[i].innerHTML='[<a href="http://groups.csail.mit.edu/locomotion/elib.cgi?b='+c+'">'+(i+1)+'</a>]';
        }
      }
    </script>
    <link rel="stylesheet" type="text/css" href="underactuated.css">
  </head>


<body data-type="book" class="book" id="htmlbook" onload="bibtex(); revealChapters();">
<section data-type="titlepage" class="titlepage">
  <header>
    <h1>Underactuated Robotics</h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p>
  	<p data-type="author"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p data-type="copyright">&copy; Russ Tedrake, 2014</p>
  </header>
</section>
<div style="display:none"> <!-- definitions for mathjax -->
  \[
  \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
  \newcommand{\bq}{{\bf q}}
  \newcommand{\bv}{{\bf v}}
  \newcommand{\bx}{{\bf x}}
  \newcommand{\by}{{\bf y}}
  \newcommand{\bu}{{\bf u}}
  \newcommand{\bw}{{\bf w}}
  \newcommand{\bz}{{\bf z}}
  \newcommand{\bK}{{\bf K}}
  \newcommand{\balpha}{{\bf \alpha}}
  \newcommand{\bbeta}{{\bf \beta}}
  \newcommand{\avg}[1]{E\left[ #1 \right]}
  \newcommand{\subjto}{\textrm{subject to}}
  \newcommand{\minimize}{\operatorname{\textrm{minimize}}}
  \newcommand{\maximize}{\operatorname{\textrm{maximize}}}
  \newcommand{\find}{\operatorname{\textrm{find}}}
  \newcommand{\argmax}{\operatorname{\textrm{argmax}}}
  \newcommand{\argmin}{\operatorname{\textrm{argmin}}}
  \newcommand{\sgn}{\operatorname{\textrm{sgn}}}
  \newcommand{\trace}{\operatorname{\textrm{tr}}}
  \newcommand{\sos}{\text{is SOS}}
  \]
</div>

<div id="debug_output"></div>

<div id="table_of_contents"></div>
<todo>generate table of contents (w/ javascript?)</todo>


<section data-type="chapter" class="chapter">
<h1>Fully-actuated vs Underactuated Systems</h1>

<p>Robots today move far too conservatively, and accomplish only a fraction of
the tasks and achieve a fraction of the performance that they are mechanically
capable of.  In many cases, we are still fundamentally limited by control
technology which matured on rigid robotic arms in structured factory
environments.  The study of underactuated robotics focuses on building control
systems which use the natural dynamics of the machines in an attempt to achieve
extraordinary performance in terms of speed, efficiency, or robustness.</p>

<section data-type="sect1">
<h1>Motivation</h1>
<p>Let's start with some examples, and some videos.</p>

<section data-type="sect2">
<h1>Honda's ASIMO vs. Passive Dynamic Walkers</h1>

<p> The world of robotics changed when, in late 1996, Honda Motor Co. announced
that they had been working for nearly 15 years (behind closed doors) on walking
robot technology.  Their designs have continued to evolve, resulting in a
humanoid robot they call ASIMO (Advanced Step in Innovative MObility).  Honda's
ASIMO is widely considered to be state of the art in walking robots, although
there are now many robots with designs and performance very similar to ASIMO's.
We will dedicate effort to understanding a few of the details of ASIMO when we
discuss algorithms for walking... for now I just want you to become familiar
with the look and feel of ASIMO's movements [watch the asimo video below now].
</p>

<figure>
  <video width="80%" controls>
    <source src="http://world.honda.com/ASIMO/technology/2011/intelligence/video02/movie640w.mp4" type="video/mp4"/>
    <source src="figures/walking_while_avoiding_people.ogg" type="video/ogg"/>
  </video><br/>
<!--
<object name="kaltura_player_1409562873" id="kaltura_player_1409562873" type="application/x-shockwave-flash" allowScriptAccess="always" allowNetworking="all" allowFullScreen="true" height="300" width="100%" data="http://www.kaltura.com/index.php/kwidget/wid/1_bybwc53n/uiconf_id/8700151?autoPlay=false"><param name="allowScriptAccess" value="always" /><param name="allowNetworking" value="all" /><param name="allowFullScreen" value="true" /><param name="bgcolor" value="#000000" /><param name="movie" value="http://www.kaltura.com/index.php/kwidget/wid/1_bybwc53n/uiconf_id/8700151"/><param name="flashVars" value=""/><a href="http://corp.kaltura.com">video platform</a><a href="http://corp.kaltura.com/video_platform/video_management">video management</a><a href="http://corp.kaltura.com/solutions/video_solution">video solutions</a><a href="http://corp.kaltura.com/video_platform/video_publishing">video player</a></object>
-->
<figcaption>Honda's ASIMO (from <a href="http://world.honda.com/ASIMO/video/">http://world.honda.com/ASIMO/video/</a>)</figcaption>

</figure>

<p> I hope that your first reaction is to be incredibly impressed with the
quality and versatility of ASIMO's movements.  Now take a second look. Although
the motions are very smooth, there is something a little unnatural about ASIMO's
gait.  It feels a little like an astronaut encumbered by a heavy space suit.  In
fact this is a reasonable analogy... ASIMO is walking a bit like somebody that
is unfamiliar with his/her dynamics.  It's control system is using high-gain
feedback, and therefore considerable joint torque, to cancel out the natural
dynamics of the machine and strictly follow a desired trajectory. This control
approach comes with a stiff penalty.  ASIMO uses roughly 20 times the energy
(scaled) that a human uses to walk on the flat (measured by cost of
transport)<elib>Collins05</elib>.  Also, control stabilization in this approach
only works in a relatively small portion of the state space (when the stance
foot is flat on the ground), so ASIMO can't move nearly as quickly as a human,
and cannot walk on unmodelled or uneven terrain. </p>

<figure>
<p>
  <video width="80%" controls title="If your browser cannot play this video, then download it using the link below.">
    <source src="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_angle.mpg" type="video/mp4"/>
    <source src="figures/passive_angle.ogg" type="video/ogg"/>
  </video><br/>
  <a href="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_angle.mpg">Download the video</a>
</p>
<p>
  <video width="80%" controls title="If your browser cannot play this video, then download it using the link below.">
    <source src="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_behind.mpg" type="video/mp4">
    <source src="figures/passive_behind.ogg" type="video/ogg"/>
  </video><br/>
  <a href="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_behind.mpg">Download the video</a>
</p>
<figcaption>A 3D passive dynamic walker by Steve Collins and Andy Ruina<elib>Collins01</elib></figcaption>
</figure>

<p> For contrast, let's now consider a very different type of walking robot,
called a passive dynamic walker (PDW).  This "robot" has no motors, no
controllers, no computer, but is still capable of walking stably down a small
ramp, powered only by gravity [watch videos above now].  Most people will agree
that the passive gait of this machine is more natural than ASIMO's; it is
certainly more efficient.  Passive walking machines have a long history - there
are patents for passively walking toys dating back to the mid 1800's.  We will
discuss, in detail, what people know about the dynamics of these machines and
what has been accomplished experimentally.  This most impressive passive dynamic
walker to date was built by Steve Collins in Andy Ruina's lab at
Cornell<elib>Collins01</elib>. </p>

<p> Passive walkers demonstrate that the high-gain, dynamics-cancelling feedback
approach taken on ASIMO is not a necessary one.  In fact, the dynamics of
walking is beautiful, and should be exploited - not cancelled out. </p>
</section>

<section data-type="sect2">
<h1>Birds vs. modern aircraft</h1>

<p> The story is surprisingly similar in a very different type of machine.
Modern airplanes are extremely effective for steady-level flight in still air.
Propellers produce thrust very efficiently, and today's cambered airfoils are
highly optimized for speed and/or efficiency. It would be easy to convince
yourself that we have nothing left to learn from birds.  But, like ASIMO, these
machines are mostly confined to a very conservative, low angle-of-attack flight
regime where the aerodynamics on the wing are well understood.  Birds routinely
execute maneuvers outside of this flight envelope (for instance, when they are
landing on a perch), and are considerably more effective than our best aircraft
at exploiting energy (eg, wind) in the air.  </p>

<p> As a consequence, birds are extremely efficient flying machines; some are
capable of migrating thousands of kilometers with incredibly small fuel
supplies.  The wandering albatross can fly for hours, or even days, without
flapping its wings - these birds exploit the shear layer formed by the wind over
the ocean surface in a technique called dynamic soaring.  Remarkably, the
metabolic cost of flying for these birds is indistinguishable from the baseline
metabolic cost<elib>Arnould96</elib>, suggesting that they can travel incredible
distances (upwind or downwind) powered almost completely by gradients in the
wind.  Other birds achieve efficiency through similarly rich interactions with
the air - including formation flying, thermal soaring, and ridge soaring.  Small
birds and large insects, such as butterflies and locusts, use `gust soaring' to
migrate hundreds or even thousands of kilometers carried primarily by the wind.
</p>

<p> Birds are also incredibly maneuverable.  The roll rate of a highly acrobatic
aircraft (e.g, the A-4 Skyhawk) is approximately 720 deg/sec<elib>Shyy08</elib>;
a barn swallow has a roll rate in excess of 5000   deg/sec<elib>Shyy08</elib>.
Bats can be flying at full-speed in one direction, and completely reverse
direction while maintaining forward speed, all in just over 2 wing-beats and in
a distance less than half the wingspan<elib>Tian06</elib>.  Although
quantitative flow visualization data from maneuvering flight is scarce, a
dominant theory is that the ability of these animals to produce sudden, large
forces for maneuverability can be attributed to unsteady aerodynamics, e.g., the
animal creates a large suction vortex to rapidly change
direction<elib>Triantafyllou95</elib>.  These astonishing capabilities are
called upon routinely in maneuvers like flared perching, prey-catching, and high
speed flying through forests and caves.  Even at high speeds and high turn
rates, these animals are capable of incredible agility - bats sometimes capture
prey on their wings, Peregrine falcons can pull 25 G's out of a 240 mph dive to
catch a sparrow in mid-flight<elib>Tucker98</elib>, and even the small birds
outside our building can be seen diving through a chain-link fence to grab a
bite of food. </p>

<p> Although many impressive statistics about avian flight have been recorded,
our understanding is partially limited by experimental accessibility - it's is
quite difficult to carefully measure birds (and the surrounding airflow) during
their most impressive maneuvers without disturbing them.  The dynamics of a
swimming fish are closely related, and can be more convenient to study.
Dolphins have been known to swim gracefully through the waves alongside ships
moving at 20 knots<elib>Triantafyllou95</elib>.  Smaller fish, such as the
bluegill sunfish, are known to possess an escape response in which they propel
themselves to full speed from rest in less than a body length; flow
visualizations indeed confirm that this is accomplished by creating a large
suction vortex along the side of the body<elib>Tytell08</elib> - similar to how
bats change direction in less than a body length. There are even observations of
a dead fish swimming upstream by pulling energy out of the wake of a cylinder;
this passive propulsion is presumably part of the technique used by rainbow
trout to swim upstream at mating season<elib>Beal06</elib>. </p>

</section>

<section data-type="sect2">
<h1>The common theme</h1>

<p> Classical control techniques for robotics are based on the idea that
feedback can be used to override the dynamics of our machines.  These examples
suggest that to achieve outstanding dynamic performance (efficiency, agility,
and robustness) from our robots, we need to understand how to design control
system which take advantage of the dynamics, not cancel them out.  That is the
topic of this course. </p>

<p> Surprisingly, there are relatively few formal control ideas that consider
"exploiting" the dynamics.  In order to convince a control theorist to consider
the dynamics (efficiency arguments are not enough), you have to do something
drastic, like taking away his control authority - remove a motor, or enforce a
torque-limit.  These issues have created a formal class of systems, the
underactuated systems, for which people have begun to more carefully consider
the dynamics of their machines in the context of control. </p>

</section>
</section>


<section data-type="sect1">
<h1>Definitions</h1>

<p> According to Newton, the dynamics of mechanical systems are second order ($F =
ma$).  Their state is given by a vector of positions, $\bq$, and a vector of
velocities, $\dot{\bq}$, and (possibly) time. The general form for a
second-order controllable dynamical system is: $$\ddot{\bq} = {\bf
f}(\bq,\dot{\bq},\bu,t),$$ where $\bu$ is the control vector.  As we will see,
the dynamics for many of the robots that we care about turn out to be affine in
commanded torque, so let's consider a slightly constrained form:
\begin{equation}\ddot{\bq} = {\bf f}_1(\bq,\dot{\bq},t) + {\bf
f}_2(\bq,\dot{\bq},t)\bu \label{eq:f1_plus_f2}.\end{equation} </p>

<div data-type="definition"><h1>Fully-Actuated</h1> A control system described
by equation \ref{eq:f1_plus_f2} is fully-actuated in configuration
$(\bq,\dot{\bq},t)$ if it is able to command an instantaneous acceleration in an
arbitrary direction in $\bq$: \begin{equation} \textrm{rank}\left[{\bf f}_2
(\bq,\dot{\bq},t)\right] = \dim\left[\bq\right].\end{equation} </div>

<div data-type="definition"><h1>Underactuated</h1> A control system described by
equation \ref{eq:f1_plus_f2} is underactuated in configuration
$(\bq,\dot{\bq},t)$ if it is not able to command an instantaneous acceleration
in an arbitrary direction in $\bq$: \begin{equation} \textrm{rank}\left[{\bf
f}_2(\bq,\dot{\bq},t)\right] < \dim\left[\bq\right].
\label{eq:underactuated_def}\end{equation} </div>

<p>Notice that whether or not a control system is underactuated may depend on
the state of the system, although for most systems (including all of the systems
in this book) underactuation is a global property of the system.</p>

<p> In words, underactuated control systems are those in which the control input
cannot accelerate the state of the robot in arbitrary directions.  As a
consequence, unlike fully-actuated systems, underactuated system cannot be
commanded to follow arbitrary trajectories.  </p>

<div data-type="example"><h1>Robot Manipulators</h1>

<figure>
<img style="width:250px;" src="figures/simple_double_pend.svg"/>
<todo>make this image spring to life with a matlab movie<</todo>
<figcaption>Simple double pendulum</figcaption>
</figure>

<p> Consider the simple robot manipulator illustrated above.  As described in
Appendix A, the equations of motion for this system are quite simple to derive,
and take the form of the standard "manipulator equations": $${\bf
H}(\bq)\ddot\bq + {\bf C}(\bq,\dot\bq)\dot\bq + {\bf G}(\bq) = {\bf
B}(\bq)\bu.$$ It is well known that the inertial matrix, ${\bf H}(\bq)$ is
(always) uniformly symmetric and positive definite, and is therefore invertible.
Putting the system into the form of equation \ref{eq:f1_plus_f2} yields:
\begin{align*}\ddot{\bq} =& -{\bf H}^{-1}(\bq)\left[ {\bf C}(\bq,\dot\bq)
\dot\bq + {\bf G}(\bq) \right]\\ &+ {\bf H}^{-1}(\bq) {\bf B}(\bq)
\bu.\end{align*} Because ${\bf H}^{-1}(\bq)$ is always full rank, we find that a
system described by the manipulator equations is fully-actuated if and only if
${\bf B}(\bq)$ is full row rank.   For this particular example, $\bq =
[\theta_1,\theta_2]^T$ and $\bu = [\tau_1,\tau_2]^T$, and ${\bf B}(\bq) = {\bf
I}_{2 \times 2}$.  The system is fully actuated.</p>

<todo>make drake elements expandable/collapseable.  nice example here: http://quhno.internetstrahlen.de/myopera/csstests/collapsible-paragraph.html</todo>
<div data-type="drake"><h1>MATLAB Example</h1>

<p> I personally learn best when I can experiment and get some physical
intuition.  The <a href="http://drake.mit.edu">companion software for the
course</a> should make it easy for you to see this   system in action.  To try
it, make sure you've installed drake, then open MATLAB and run
<code>addpath_drake</code> (possibly from your <code>startup.m</code>) in the
<code>drake</code> directory, then try the following lines (one at a time) from
the MATLAB command line:

<pre><code class="matlab">
% a platform independent way to cd into the example directory
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));

% first construct the "robot" or "plant" object
plant = DoublePendPlant;  % check out the code in DoublePendPlant.m

% a visualizer will draw the robot
visualizer = DoublePendVisualizer(plant);
visualizer.inspector();  % (optional) simple gui to examine the joints

% simulate the robot from time=0 to time=5 seconds from random
% initial conditions taken from a Gaussian distribution (using randn)
trajectory = simulate(plant, [0 5], randn(4,1));

% play back the simulation using the cpu clock to get the timing right
visualizer.playback(trajectory);

% if you want to access the manipulator equations programmatically,
% you can use, e.g.:
[H,C_times_v,G,B] = plant.manipulatorEquations()
% which outputs using the abbreviations v for qdot, s1 for sin(q1), etc
</code></pre>
</p>
</div>

<div data-type="drake"><h1>MATLAB Example (an even simpler version)</h1>

<p>Note that you don't have to write the dynamics yourself.  The software
supports a high-level description format for describing robots.  We could have
alternatively done: </p>

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));

% construct the robot from a URDF high-level description file
plant = RigidBodyManipulator('SimpleDoublePendulum.urdf');
visualizer = plant.constructVisualizer();

% all of the other commands will work, e.g.:
trajectory = simulate(plant, [0 5], randn(4,1));
visualizer.playback(trajectory);
[H,C_times_v,G,B] = plant.manipulatorEquations()
</code></pre>

<p>Make sure you take a peek at the file <code>SimpleDoublePendulum.urdf</code>.
Documentation for the input format is available on the <a
href="http://drake.mit.edu">drake wiki</a>; it supports even quite complex
robots.</p>
</div>

<p> While the basic double pendulum is fully actuated, imagine the somewhat
bizarre case that we have a motor to provide torque at the elbow, but no motor
at the shoulder.  In this case, we have $\bu = \tau_2$, and ${\bf B}(\bq) =
[0,1]^T$.  This system is clearly underactuated.  While it may sound like a
contrived example, it turns out that it is almost exactly the dynamics we will
use to study as our simplest model of walking later in the class. </p>

</div>

<p>The matrix ${\bf f}_2$ is equation \ref{eq:f1_plus_f2} always has dim$[\bq]$
rows, and dim$[\bu]$ columns. Therefore, as in the example, one of the most
common cases for underactuation, which trivially implies that ${\bf f}_2$ is not
full row rank, is dim$[\bu] < $ dim$[\bq]$.  This is the case when a robot has
joints with no motors. But this is not the only case.  The human body, for
instance, has an incredible number of actuators (muscles), and in many cases has
multiple muscles per joint; despite having more actuators that position
variables, when I jump through the air, there is no combination of muscle inputs
that can change the ballistic trajectory of my center of mass (barring
aerodynamic effects).  My control system is underactuated.</p>

<p>For completeness, let's generalize the definition of underactuation to
systems beyond the second-order control affine systems.

<div data-type="definition"> <h1>Underactuated Control Differential
Equations</h1> An $n$th-order control differential equation control described by
the equations \begin{equation} \frac{d^n{\bf q}}{dt^n} = f({\bf q}, ...,
\frac{d^{n-1} {\bf q}}{dt^{n-1}}, t, {\bf u}) \end{equation} is fully actuated
in state ${\bf x} = ({\bf q}, ..., \frac{d^{n-1} {\bf q}}{dt^{n-1}})$ and time
$t$ if the resulting map $f$ is surjective: for every $\frac{d^n{\bf q}}{dt^n} $
there exists a ${\bf u}$ which produces the desired response.  Otherwise it is
underactuated. </div>

It is easy to see that equation \ref{eq:underactuated_def} is a sufficient
condition for underactuation.  This definition can also be extended to
discrete-time systems and/or differential inclusions. </p>

<p>A quick note about notation.  When describing the dynamics of rigid-body
systems in this class, I will use $\bq$ for positions, $\dot{\bq}$ for
velocities, and use $\bx$ for the full state ($\bx = [\bq,\dot{\bq}]^T$).  There
is an important limitation to this convention, described in the Appendix<!--
ch:robot_dynamics -->, but it will keep the notes more clean. Unless otherwise
noted, vectors are always treated as column vectors. Vectors and matrices are
bold (scalars are not). </p>

</section> <section data-type="sect1"> <h1>Feedback Linearization</h1>

<p> Fully-actuated systems are dramatically easier to control than underactuated
systems.  The key observation is that, for fully-actuated systems with known
dynamics (e.g., ${\bf f}_1$ and ${\bf f}_2$ are known), it is possible to use
feedback to effectively change a nonlinear control problem into a linear control
problem.  The field of linear control is incredibly advanced, and there are many
well-known solutions for controlling linear systems. </p>

<p>The trick is called feedback linearization.  When ${\bf f}_2$ is full row
rank, it is invertible.  Consider the nonlinear feedback law: $$\bu = {\bf
\pi}(\bq,\dot\bq,t) = {\bf f}_2^{-1}(\bq,\dot\bq,t) \left[ \bu' - {\bf
f}_1(\bq,\dot\bq,t) \right],$$ where $\bu'$ is some additional control input.
Applying this feedback controller to equation~\ref{eq:f1_plus_f2} results in the
linear, decoupled, second-order system: $$\ddot{\bq} = \bu'.$$ In other words,
if ${\bf f}_1$ and ${\bf f}_2$ are known and ${\bf f}_2$ is invertible, then we
say that the system is "feedback equivalent" to $\ddot{\bq} = \bu'$.  There are
a number of strong results which generalize this idea to the case where ${\bf
f}_1$ and ${\bf f}_2$ are estimated, rather than known (e.g,
<elib>Slotine90</elib>).</p>

<div data-type="example"><h1>Feedback-Cancellation Double Pendulum</h1>

<p> Let's say that we would like our simple double pendulum to act like a simple
single pendulum (with damping), whose dynamics are given by: \begin{align*}
\ddot \theta_1 &= -\frac{g}{l}\cos\theta_1 -b\dot\theta_1 \\ \ddot\theta_2 &= 0.
\end{align*} This is easily achieved using <sidenote>Note that our chosen
dynamics do not actually stabilize $\theta_2$ - this detail was left out for
clarity, but would be necessary for any real implementation.</sidenote> $$\bu =
{\bf B}^{-1}\left[ {\bf C}\dot{\bq} + {\bf G} + {\bf H}\begin{bmatrix}
-\frac{g}{l}c_1 - b\dot{q}_1 \\ 0 \end{bmatrix} \right].$$ </p>

<p> Since we are embedding a nonlinear dynamics (not a linear one), we refer to
this as "feedback cancellation", or "dynamic inversion".  This idea can, and
does, make control look easy - for the special case of a fully-actuated
deterministic system with known dynamics.  For example, it would have been just
as easy for me to invert gravity. Observe that the control derivations here
would not have been any more difficult if the robot had 100 joints. </p>

<div data-type="drake"><h1>MATLAB Example</h1> You can find scripts which
implement both of these examples in drake:

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));
runSimplePend;    % feedback linearization to simulate a pendulum
runSimplePendInv; % and a gravity-inverted pendulum
</code></pre>
</div>
</div>

<p> The underactuated systems are not feedback linearizable.  Therefore, unlike
fully-actuated systems, the control designer has no choice but to reason about
the nonlinear dynamics of the plant in the control design.  This dramatically
complicates feedback controller design. </p>

</section>

<section data-type="sect1"><h1>Input and State Constraints</h1>

<p> Although the dynamic constraints due to missing actuators certainly embody
the spirit of this course, many of the systems we care about could be subject to
other dynamic constraints, as well.  For example, the actuators on our machines
may only be mechanically capable of producing some limited amount of torque, or
their may be a physical obstacle in the free space which we cannot permit our
robot to come into contact with. </p>

<div data-type="definition"><h1>Input and State Constraints</h1> A dynamical
system described by $\dot{\bx} = {\bf f}(\bx,\bu,t)$ may be subject to one or
more constraints described by ${\bf \phi}(\bx,\bu,t)\le0$. </div>

<p>In practice it can useful to separate out constraints which depend only on
the input, e.g. $\phi(\bu)\le0$, such as actuator limits, as they can often be
easier to handle than state constraints.  An obstacle in the environment might
manifest itself as one or more constraints that depend only on position, e.g.
$\phi(\bq)\le0$.</p>

<p>By our generalized definition of underactuation, we can see that input and
state constraints can also cause a system to be underactuated. <div
data-type="example"><h1>Input limits</h1> Consider the constrained second-order
linear system \[ \ddot{x} = u, \quad |u| \le 1. \] By our definition, this
system is underactuated.  For example, there is no $u$ which can produce the
acceleration $\ddot{x} = 2$. </div> Input and state constraints can complicate
control design in similar ways to having an insufficient number of actuators,
(i.e., the robot cannot follow arbitrary trajectories), and often require
similar tools to find a control solution.</p>

<section data-type="sect2"><h1>Nonholonomic constraints</h1>

<p> You might have heard of the term "nonholonomic system" (e.g.
<elib>Bloch03</elib>), and be thinking about how nonholonomy relates to
underactuation.  Briefly, a nonholonomic constraint is a constraint on the
derivatives of the system which cannot be integrated into a constraint on the
positions of the system.  An automobile or traditional wheeled robot provides a
canonical example:

<div data-type="example"><h1>Wheeled robot</h1> Consider a simple model of a
wheeled robot who's configuration is described by its Cartesian position $x,y$
and its orientation, $\theta$, so ${\bf q} = \begin{bmatrix} x, y, \theta
\end{bmatrix}^T$.  The system is subject to differential constraints that
prevent side-slip, e.g., \begin{gather*} \dot{x} = v \cos\theta \\ \dot{y} = v
\sin\theta \\ v = \sqrt{\dot{x}^2 + \dot{y}^2} \end{gather*} These constraints
cannot be integrated into constraint on position--the car can get to any
configuration $(x,y,\theta)$, it just can't move directly sideways--so they are
nonholonomic constraints. </div>

Contrast the wheel robot example with a robot on train tracks.  The train tracks
would represent a holonomic constraint: if the system was written down in the
$(x,y,\theta)$ coordinates then the differential track constraints could be
integrated resulting in a description of the system in a reduced coordinate
system--train tracks are holonomic constraints. </p>

<p> A nonholomic constraint like the no-side-slip constraint on the wheeled
vehicle certainly results in an underactuated system.  The converse is not
necessarily true--the double pendulum system which is missing an actuator is
underactauted but would not typically be called a nonholonmic system.  However,
while the definition of a nonholonomic constraint is clear, in my view the
definition used in the literature for a "nonholonomic system" is more
ambiguous&dagger;<sidenote>&dagger; Why, for instance, are the Lagrangian
equations of motion for the double pendulum--clearly non-integrable-- not
considered nonholonomic constraints?</sidenote>, and we'll generally avoid using
the term. </p>

</section>
</section>

<section data-type="sect1"><h1>Underactuated robotics</h1>

<p> The control of underactuated systems is an open and interesting problem in
controls - although there are a number of special cases where underactuated
systems have been controlled, there are relatively few general principles.  Now
here's the rub... most of the interesting problems in robotics are
underactuated: </p>

<ul>

<li> Legged robots are underactuated.  Consider a legged machine with $N$
internal joints and $N$ actuators.  If the robot is not bolted to the ground,
then the degrees of freedom of the system include both the internal joints and
the six degrees of freedom which define the position and orientation of the
robot in space.  Since $\bu \in \Re^N$ and $\bq \in \Re^{N+6}$, equation
\ref{eq:underactuated_def} is satisfied.</li>

<li> (Most) Swimming and flying robots are underactuated.  The story is the same
here as for legged machines.  Each control surface adds one actuator and one
DOF.  And this is already a simplification, as the true state of the system
should really include the (infinite-dimensional) state of the flow.</li>

<li> Robot manipulation is (often) underactuated.  Consider a fully-actuated
robotic arm.  When this arm is manipulating an object w/ degrees of freedom
(even a brick has six), it can become underactuated.  If force closure is
achieved, and maintained, then we can think of the system as fully-actuated,
because the degrees of freedom of the object are constrained to match the
degrees of freedom of the hand.  That is, of course, unless the manipulated
object has extra DOFs.</li>

</ul>

<p> Even fully-actuated and otherwise unconstrained control systems can be
improved using the lessons from underactuated systems, particularly if there is
a need to increase the efficiency of their motions or reduce the complexity of
their designs.</p>

</section>

<section data-type="sect1"><h1>Goals for the course</h1>

<p> This course is based on the observation that there are new computational
tools from optimization theory, control theory, motion planning, and even
machine learning which be used to design feedback control for underactuated
systems.  The goal of this class is to develop these tools in order to design
robots that are more dynamic and more agile than the current
state-of-the-art.</p>

<p> The target audience for the class includes both computer science and
mechanical/aero students pursuing research in robotics.  Although I assume a
comfort with linear algebra, ODEs, and MATLAB, the course notes will provide
most of the material and references required for the course. </p>

</section>

</section> <!-- end chapter -->

<section data-type="chapter" class="chapter"><h1>The Simple Pendulum</h1>

<section data-type="sect1"><h1>Introduction</h1>

<p> Our goals for this chapter are modest: we'd like to understand the dynamics
of a pendulum.</p>

<p>Why a pendulum?  In part, because the dynamics of a majority of our
multi-link robotics manipulators are simply the dynamics of a large number of
coupled pendula.  Also, the dynamics of a single pendulum are rich enough to
introduce most of the concepts from nonlinear dynamics that we will use in this
text, but tractable enough for us to (mostly) understand in the next few pages.
</p>

<figure> <img width="30%" src="figures/simple_pend.svg"/> <figcaption>The simple
pendulum</figcaption> </figure>

<p> The Lagrangian derivation of the equations of motion (as described in the
appendix) of the simple pendulum yields: \begin{equation*} m l^2 \ddot\theta(t) +
mgl\sin{\theta(t)} = Q. \end{equation*} We'll consider the case where the
generalized force, $Q$, models a damping torque (from friction) plus a control
torque input, $u(t)$: $$Q = -b\dot\theta(t) + u(t).$$ </p>

</section>

<section data-type="sect1"><h1>Nonlinear Dynamics w/ a Constant Torque</h1>

<p> Let us first consider the dynamics of the pendulum if it is driven in a
particular simple way: a torque which does not vary with time: \begin{equation}
ml^2 \ddot\theta + b\dot\theta + mgl \sin\theta = u_0. \end{equation}
You can experiment with this system in <em>drake</em> using

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Pendulum'));
open_system('constantTorqueDemo');
% hit play to start the simulation, and click on the boxes
% labeled as 'slider' to control the torque and damping
</code></pre>

These are relatively simple differential equations, so if I give you $\theta(0)$ and
$\dot\theta(0)$, then you should be able to integrate them to obtain
$\theta(t)$... right?  Although it is possible, integrating even the simplest
case ($b = u = 0$) involves elliptic integrals of the first kind; there is
relatively little intuition to be gained here. </p>

<p> This is in stark contrast to the case of linear systems, where much of
our understanding comes from being able to explictly integrate the equations.
For instance, for a simple linear system we have $$\dot{q} = a q \quad
\rightarrow \quad q(t) = q(0) e^{at},$$ and we can immediately understand that
the long-term behavior of the system is a (stable) decaying exponential if
$a<0$, an (unstable) growing expontential if $a>0$, and that the system does
nothing if $a=0$. Here we are with certainly one of the simplest nonlinear
systems we can imagine, and we can't even solve this system? </p>

<p> All is not lost.  If what we care about is the long-term behavior of the
system, then there are a number of techniques we can apply.  In this chapter, we
will start by investigating graphical solution methods. These methods are
described beautifully in a book by Steve Strogatz<elib>Strogatz94</elib>.

<section data-type="sect2"><h1>The Overdamped Pendulum</h1>

<p> Let's start by studying a special case -- intuitively when $b \gg ml^2$, but
to get the units right we actually need to include a fundamental time constant,
so $\frac{b}{ml^2} \sqrt\frac{g}{l} \gg 1$. This is the case of heavy damping,
for instance if the pendulum was moving in molasses.  In this case, the damping
term dominates the acceleration term, and we have: $$ml^2 \ddot\theta +
b\dot\theta \approx b\dot\theta = u_0 - mgl\sin\theta.$$ In other words, in the
case of heavy damping, the system looks approximately first-order. This is a
general property of heavily damped systems, such as fluids at very low Reynolds
number. </p>

<p> I'd like to ignore one detail for a moment: the fact that $\theta$ wraps
around on itself every $2\pi$.  To be clear, let's write the system without the
wrap-around as: \begin{equation}b\dot{x} = u_0 -
mgl\sin{x}.\label{eq:overdamped_pend_ct}\end{equation} Our goal is to understand
the long-term behavior of this system: to find $x(\infty)$ given $x(0)$.  Let's
start by plotting $\dot{x}$ vs $x$ for the case when $u_0=0$:

<figure> <img width="70%" src="figures/pend_sinx.svg"/> </figure> </p>

<p> The first thing to notice is that the system has a number of <em>fixed
points</em> or <em>steady states</em>, which occur whenever $\dot{x} = 0$.  In
this simple example, the zero-crossings are $x^* = \{..., -\pi, 0, \pi, 2\pi, ...\}$.
When the system is in one of these states, it will never leave that state.  If
the initial conditions are at a fixed point, we know that $x(\infty)$ will be at
the same fixed point.</p>

<p>Next let's investigate the behavior of the system in the local vicinity of
the fixed points.  Examing the fixed point at $x^* = \pi$, if the system starts
just to the right of the fixed point, then $\dot{x}$ is positive, so the system
will move away from the fixed point.  If it starts to the left, then $\dot{x}$
is negative, and the system will move away in the opposite direction.  We'll
call fixed-points which have this property <em>unstable</em>.  If we look at the
fixed point at $x^* = 0$, then the story is different: trajectories starting to
the right or to the left will move back towards the fixed point.  We will call
this fixed point <em>locally stable</em>.  More specifically, we'll distinguish
between three types of local stability:

<ul>

<li>Locally stable <em>in the sense of Lyapunov</em> (i.s.L.).  A fixed point, $x^*$ is
locally stable i.s.L. if for every small $\epsilon$, I can produce a $\delta$
such that if $\| x(0) - x^* \| < \delta$ then $\forall t$ $\| x(t) - x^*\| <
\epsilon$.  In words, this means that for any ball of size $\epsilon$ around the
fixed point, I can create a ball of size $\delta$ which guarantees that if the
system is started inside the $\delta$ ball then it will remain inside the
$\epsilon$ ball for all of time.</li>

<li>Locally <em>asymptotically stable</em>.  A fixed point is locally asymptotically
stable if it is stable i.s.L. and $x(0) = x^* + \epsilon$ implies that $\lim_{t\rightarrow \infty} x(t) = x^*$.</li>

<li>Locally <em>exponentially stable</em>.  A fixed point is locally exponentially stable
if $x(0) = x^* + \epsilon$ implies that $\| x(t) - x^* \| < Ce^{-\alpha t}$, for
some positive constants $C$ and $\alpha$.</li>

</ul>

An initial condition near a fixed point that is stable in the sense of Lyapunov
may never reach the fixed point (but it won't diverge), near an asymptotically
stable fixed point will reach the fixed point as $t \rightarrow \infty$, and
near an exponentially stable fixed point will reach the fixed point with a
bounded rate.  An exponentially stable fixed point is also an asymptotically
stable fixed point, and an asymptotically stable fixed point is also stable (by
definition<!-- see
http://books.google.com/books?id=ByCCAwAAQBAJ&pg=PA6&lpg=PA6&dq=locally+attractive+but+not++lyapunov+stable&source=bl&ots=BLlwzfFryl&sig=lDXX4oFzIhXEH7do-4llBXU3NKo&hl=en&sa=X&ei=tqEVVKDEGoqYyATh84G4Cw&ved=0CDkQ6AEwAw#v=onepage&q=locally%20attractive%20but%20not%20%20lyapunov%20stable&f=false
for a counter-example -->) but the converse of these is not necessarily
true. Interestingly, it is also possible to have nonlinear systems that converge
(or diverge) in finite-time; a so-called <em>finite-time stability</em>.  We
will see examples of this later in the book, but it is a difficult topic to
penetrate with graphical analysis.</p>

<p> Our graph of $\dot{x}$ vs. $x$ can be used to convince ourselves of i.s.L.
and asymptotic stability by visually inspecting $\dot{x}$ in the vicinity of a
fixed point.  Even exponential stability can be inferred if the function can be
bounded away from the origin by a negatively-sloped line through the fixed
point, since it implies that the nonlinear system will converge at least as fast
as the linear system represented by the straight line.  I will graphically
illustrate unstable fixed points with open circles and stable fixed points
(i.s.L.) with filled circles. </p>

<p>Next, we need to consider what happens to initial conditions which begin
farther from the fixed points.  If we think of the dynamics of the system as a
flow on the $x$-axis, then we know that anytime $\dot{x} > 0$, the flow is
moving to the right, and $\dot{x} < 0$, the flow is moving to the left.  If we
further annotate our graph with arrows indicating the direction of the flow,
then the entire (long-term) system behavior becomes clear:

<figure> <img width="70%" src="figures/pend_sinx_annotated.svg"/> </figure>

For instance, we can see that any initial condition $x(0) \in (\pi,\pi)$ will
result in $\lim_{t\rightarrow \infty} x(t) = 0$.  This region is called the <em>basin of
attraction</em> of the fixed point at $x^* = 0$. Basins of attraction of two
fixed points cannot overlap, and the manifold separating two basins of
attraction is called the <em>separatrix</em>.  Here the unstable fixed points,
at $x^* = \{.., -\pi, \pi, 3\pi, ...\}$ form the separatrix between the basins
of attraction of the stable fixed points. </p>

<p> As these plots demonstrate, the behavior of a first-order one dimensional
system on a line is relatively constrained.  The system will either
monotonically approach a fixed-point or monotonically move toward $\pm \infty$.
There are no other possibilities.  Oscillations, for example, are impossible.
Graphical analysis is a fantastic analysis tool for many first-order nonlinear
systems (not just pendula); as illustrated by the following example: </p>

<div data-type="example"><h1>Nonlinear autapse</h1> Consider the following
system: \begin{equation} \dot{x} + x = \tanh(w x), \end{equation} which is
plotted below for a few values of $w$.  It's convenient to note that $\tanh(z)
\approx z$ for small $z$.  For $w \le 1$ the system has only a single fixed
point.  For $w > 1$ the system has three fixed points : two stable and one
unstable.

<figure> <img width="80%" src="figures/pend_autapse.svg"/> </figure>

These equations are not arbitrary - they are actually a model for one of the
simplest neural networks, and one of the simplest model of persistent
memory<elib>Seung00</elib>.  In the equation $x$ models the firing rate of a
single neuron, which has a feedback connection to itself.  $\tanh$ is the
activation (sigmoidal) function of the neuron, and $w$ is the weight of the
synaptic feedback. </div>

<p> One last piece of terminology.  In the neuron example, and in many dynamical
systems, the dynamics were parameterized; in this case by a single parameter,
$w$.  As we varied $w$, the fixed points of the system moved around.  In fact,
if we increase $w$ through $w=1$, something dramatic happens - the system goes
from having one fixed point to having three fixed points.  This is called a
<em>bifurcation</em>.  This particular bifurcation is called a pitchfork
bifurcation.  We often draw bifurcation diagrams which plot the fixed points of
the system as a function of the parameters, with solid lines indicating stable
fixed points and dashed lines indicating unstable fixed points, as seen in the
figure:</p>

<figure> <todo>bifurcation diagram asymptotes to $x^* = 1$</todo> <img
width="80%" src="figures/pend_autapse_bifurcation.svg"/> <figcaption>Bifurcation
diagram of the nonlinear autapse.</figcaption> </figure>

<p> Our pendulum equations also have a (saddle-node) bifurcation when we change
the constant torque input, $u_0$.  <!-- This is the subject of
exercise~\ref{p:pend_bifurcation}.-->  Finally, let's return to the original
equations in $\theta$, instead of in $x$.  Only one point to make: because of
the wrap-around, this system will <em>appear</em> have oscillations.  In fact,
the graphical analysis reveals that the pendulum will turn forever whenever
$|u_0| > mgl$, but now you understand that this is not an oscillation, but an
instability with $\theta \rightarrow \pm \infty$. </p>

</section> <!-- end of overdamped pend -->

<section data-type="sect2"><h1>The Undamped Pendulum w/ Zero Torque</h1>

<p> Consider again the system $$ml^2 \ddot\theta = u_0 - mgl \sin\theta -
b\dot\theta,$$ this time with $b = 0$.  This time the system dynamics are truly
second-order.  We can always think of any second-order system as (coupled)
first-order system with twice as many variables. Consider a general, autonomous
(not dependent on time), second-order system, $$\ddot{q} = f(q,\dot q,u).$$ This
system is equivalent to the two-dimensional first-order system \begin{align*}
\dot x_1 =& x_2 \\ \dot x_2 =& f(x_1,x_2,u), \end{align*} where $x_1 = q$ and
$x_2 = \dot q$.  Therefore, the graphical depiction of this system is not a
line, but a vector field where the vectors $[\dot x_1, \dot x_2]^T$ are plotted
over the domain $(x_1,x_2)$.  This vector field is known as the <em>phase
portrait</em> of the system.</p>

<p> In this section we restrict ourselves to the simplest case when $u_0 = 0$.
Let's sketch the phase portrait.  First sketch along the $\theta$-axis. The
$x$-component of the vector field here is zero, the $y$-component is
$-mgl\sin\theta.$ As expected, we have fixed points at $\pm \pi, ...$ Now sketch
the rest of the vector field.  Can you tell me which fixed points are stable?
Some of them are stable i.s.L., none are asymptotically stable.

<figure> <img width="80%" src="figures/pend_undamped_phase.svg"/> </figure> </p>

<section data-type="sect3"><h1>Orbit Calculations</h1>

<p> You might wonder how we drew the black countour lines in the figure above.  We could
have obtained them by simulating the system numerically, but those lines can be
easily obtained in closed-form.  Directly integrating the equations of motion is
difficult, but at least for the case when $u_0 = 0$, we have some additional
physical insight for this problem that we can take advantage of.  The kinetic
energy, $T$, and potential energy, $U$, of the pendulum are given by $$T =
\frac{1}{2}I\dot\theta^2, \quad U = -mgl\cos(\theta),$$ where $I=ml^2$ and the
total energy is $E(\theta,\dot\theta) = T(\dot\theta)+U(\theta)$.  The undamped
pendulum is a conservative system: total energy is a constant over system
trajectories.  Using conservation of energy, we have:

\begin{gather*}  E(\theta(t),\dot\theta(t)) = E(\theta(0),\dot\theta(0)) = E_0 \\
\frac{1}{2} I \dot\theta^2(t) - mgl\cos(\theta(t)) = E_0  \\ \dot\theta(t) = \pm
\sqrt{\frac{2}{I}\left[E_0 + mgl\cos\left(\theta(t)\right)\right]} \end{gather*}

Using this, if you tell me $\theta$ I can determine one of two possible values
for $\dot\theta$, and the solution has all of the richness of the black countour
lines from the plot.  This equation has a real solution when $\cos(\theta) >
\cos(\theta_{max})$, where $$\theta_{max} = \begin{cases} \cos^{-1}\left(
\frac{E}{mgl} \right), & E < mgl \\ \pi, & \text{otherwise}. \end{cases}$$ Of
course this is just the intuitive notion that the pendulum will not swing above
the height where the total energy equals the potential energy.  As an exercise,
you can verify that differentiating this equation with respect to time indeed
results in the equations of motion.</p>

</section>
<section data-type="sect3"><h1>Trajectory Calculations</h1>

<p> For completeness, I'll include what it would take to solve for $\theta(t)$,
even thought it cannot be accomplished using elementary functions.  Feel free to
skip this subsection.  We begin the integration with

\begin{gather*} \frac{d\theta}{dt} = \sqrt{\frac{2}{I}\left[E +
mgl\cos\left(\theta(t)\right)\right]} \\ \int_{\theta(0)}^{\theta(t)}
\frac{d\theta}{\sqrt{\frac{2}{I}\left[E +
mgl\cos\left(\theta(t)\right)\right]}} = \int_0^t dt' = t \end{gather*}

The
integral on the left side of this equation is an (incomplete) elliptic integral
of the first kind.  Using the identity: $$\cos(\theta) = 1 - 2
\sin^2(\frac{1}{2}\theta),$$ and manipulating, we have $$t =
\sqrt{\frac{I}{2(E+mgl)}} \int_{\theta(0)}^{\theta(t)} \frac{d\theta}{\sqrt{1 -
k_1^2\sin^2(\frac{\theta}{2})}}, \quad \text{with
}k_1=\sqrt{\frac{2mgl}{E+mgl}}.$$ In terms of the incomplete elliptic integral
function, $$F(\phi,k) = \int_0^\phi \frac{d\theta}{\sqrt{1-k^2\sin^2\theta}},$$
accomplished by a change of variables.  If $E <= mgl$, which is the case of
closed-orbits, we use the following change of variables to ensure $ 0 < k < 1 $ :
\begin{gather*}\phi = \sin^{-1}\left[ k_1 \sin\left( \frac{\theta}{2} \right)
\right] \\ \cos(\phi) d\phi = \frac{1}{2} k_1 \cos\left(\frac{\theta}{2}\right)
d\theta = \frac{1}{2} k_1 \sqrt{1 - \frac{\sin^2 (\phi)}{k_1^2}} d\theta
\end{gather*} so we have
\begin{gather*}
t = \frac{1}{k_1}\sqrt{\frac{2I}{(E+mgl)}} \int_{\phi(0)}^{\phi(t)}
\frac{d\phi}{\sqrt{1 - \sin^2(\phi)}} \frac{\cos(\phi)}{\sqrt{1 -
\frac{\sin^2\phi}{k_1^2}}} \\ = \sqrt{\frac{I}{mgl}} \left[
F\left(\phi(t),k_2\right) - F\left(\phi(0),k_2\right) \right],\quad
k_2 = \frac{1}{k_1}.\end{gather*} The inverse of $F$ is given by the
Jacobi elliptic functions (sn,cn,...), yielding:
<!-- http://en.wikipedia.org/wiki/Jacobi%27s_elliptic_functions -->
\begin{gather*}\sin(\phi(t)) = \text{sn}\left(t
  \sqrt{\frac{mgl}{I}} + F\left(\phi(0),k_2\right),k_2 \right) \\
\theta(t) = 2\sin^{-1} \left[ k_2 \text{sn}\left(t
  \sqrt{\frac{mgl}{I}} + F\left(\phi(0),k_2\right),k_2 \right) \right]
\end{gather*}
The function $\text{sn}$ used here can be evaluated in MATLAB by
calling $$\text{sn}(u,k) = \text{ellipj}(u,k^2).$$ The function
$F$ is not implemented in MATLAB, but implementations can be
downloaded. (note that $F(0,k) = 0$).</p>

<p>
For the open-orbit case, $E>mgl$, we use $$\phi =
\frac{\theta}{2},\quad \frac{d\phi}{d\theta} = \frac{1}{2},$$ yielding
\begin{gather*}
t = \frac{2I}{E+mgl} \int_{\phi(0)}^{\phi(t)} \frac{d\phi}{\sqrt{1 -
    k_1^2 \sin^2(\phi)}} \\
\theta(t) = 2 \tan^{-1} \left[ \frac{ \text{sn} \left( t
    \sqrt{\frac{E+mgl}{2I}} + F\left( \frac{\theta(0)}{2}, k_1 \right)
    \right) } { \text{cn} \left( t
    \sqrt{\frac{E+mgl}{2I}} + F\left( \frac{\theta(0)}{2}, k_1 \right)
    \right) }
 \right]
\end{gather*}
Notes: Use MATLAB's <code>atan2</code> and <code>unwrap</code> to recover the
complete trajectory.</p>

<!-- primary refs:
  http://en.wikipedia.org/wiki/Pendulum_%28mathematics%29
  http://kr.cs.ait.ac.th/~radok/math/mat6/calc81.htm
  http://books.google.com/books?id=xWrJlTIYl_IC&pg=PA280&lpg=PA280&dq=pendulum+elliptic+integrals&source=web&ots=zas9k5qVc0&sig=Tz-OwdqS84VPM2I_pZbtaspVZNk#PPA280,M1 (Computational Physics: Problem Solving with Computers by Rubin H. Landau, Cristian C. Bordeianu, p.280)
  http://en.wikipedia.org/wiki/Binomial_theorem
  http://mathworld.wolfram.com/EllipticIntegraloftheFirstKind.html -->


</div>
</section>

</section> <!-- end of undamped pend -->

<section data-type="sect2"><h1>The Undamped Pendulum w/ a Constant Torque</h1>

<p> Now what happens if we add a constant torque?  If you visualize the
bifurcation diagram, you'll see that the fixed points come together, towards $q =
\frac{\pi}{2}, \frac{5\pi}{2}, ...$, until they disappear.  One fixed-point is
unstable, and one is stable.</p>

<!-- todo: find a way to add an animation or something in here.  do a matlab demo in class -->

</section> <!-- undamped constant torque -->

<section data-type="sect2"><h1>The Dampled Pendulum</h1>

<p>
Now let's add damping back.  You can still add torque to move the fixed points
(in the same way).</p>

<figure>
  <img width="90%" src="figures/pend_damped_phase.svg"/>
  <figcaption>Phase diagram for the damped pendulum</figcaption>
</figure>

<p> Here's a thought exercise.  If $u$ is no longer a constant, but a function
$\pi(q,\dot{q})$, then how would you choose $\pi$ to stabilize the vertical
position.  Feedback linearization is the trivial solution, for example: $$u =
\pi(q,\dot{q}) = 2 \frac{g}{l}\cos\theta.$$ But these plots we've been making
tell a different story.  How would you shape the natural dynamics - at each
point pick a $u$ from the stack of phase plots - to stabilize the vertical fixed
point <em>with minimal torque effort</em>?  This is exactly the way that I would
like you to think about control system design.  And we'll give you your first solution techniques -- using dynamic programming -- in the next lecture.</p>

</section> <!-- damped pend -->

</section> <!-- end of constant torque -->

<section data-type="sect1"><h1>The Torque-limited Simple Pendulum</h1>

<p>
The simple pendulum is fully actuated.  Given enough torque, we can
produce any number of control solutions to stabilize the originally
unstable fixed point at the top (such as designing a feedback law to
effectively invert gravity).</p>


<p>   The problem begins to get interesting (a.k.a. becomes underactuated) if we
impose a torque-limit constraint, $|u|\le u_{max}$.  Looking at the phase
portraits again, you can now visualize the control problem.  Via feedback, you
are allowed to change the direction of the vector field at each point, but only
by a fixed amount.  Clearly, if the maximum torque is small (smaller than
$mgl$), then there are some states which cannot be driven directly to the goal,
but must pump up energy to reach the goal.  Futhermore, if the torque-limit is
too severe and the system has damping, then it may be impossible to swing up to
the top.  The existence of a solution, and number of pumps required to reach the
top, is a non-trivial function of the initial conditions and the
torque-limits.</p>

<p> Although this system is very simple, its solution requires much of the same
reasoning necessary for controlling much more complex underactuated systems;
this problem will be a work-horse for us as we introduce new algorithms
throughout this book.</p>

</section>


</section>

 <!-- ***************  end of pendulum chapter **************   -->

<section data-type="chapter" class="chapter"><h1>Acrobots, Cart-Poles, and Quadrotors</h1>

</section>

<!-- ***************  end of acrobot/cartpole/quadrotor chapter **************   -->


<section data-type="chapter" class="chapter"><h1>Walking and Running</h1>

</section>

<!-- ***************  end of walking and running **************   -->

<section data-type="chapter" class="chapter"><h1>Manipulation</h1>

</section>

<!-- ***************  end of Manipulation **************   -->

<section data-type="chapter" class="chapter"><h1>Model Systems with Fluid Dynamics</h1>

</section>

<!-- ***************  end of fluid dynamics **************   -->

<section data-type="chapter" class="chapter"><h1>Model Systems with Stochasticity</h1>

</section>

<!-- ***************  end of stochasticity **************   -->

<!-- START PART II:  Nonlinear Planning and Control -->

<section data-type="chapter" class="chapter"><h1>Dynamic Programming</h1>

<p>In chapter 2, we spent some time thinking about the phase portrait of the
simple pendulum, and concluded with a challenge: can we design a nonlinear
controller to <em>reshape</em> the phase portrait, with a very modest amount of
actuation, so that the upright fixed point becomes globally stable?  With
unbounded torque, feedback linearization solutions (e.g., invert gravity) can
work well, but can also require an unecessarily large amount of control effort.
The energy-based swing-up control solutions presented for the acrobot and
cart-pole systems  are considerably more appealing, but required some cleverness
and might not scale to more complicated systems.  Here we investigate another
approach to the problem, using computational optimal control to synthesize a
feedback controller directly.</p>

<section data-type="sect1"><h1>Formulating control design as an optimization</h1>

<p>   In this chapter, we will introduce optimal control - a control design
process using optimization.  This approach is powerful for a number of reasons.
First and foremost, it is very general - allowing us to   specify the goal of
control equally well for fully- or under-actuated,   linear or nonlinear,
deterministic or stochastic, and continuous or   discrete systems. Second, it
permits concise descriptions of potentially very complex desired behaviours,
specifying the goal of control as an scalar objective (plus a list of
constraints).  Finally, and most importantly, optimal control is very amenable
to numerical solutions. <elib>Bertsekas00a</elib> is a fantastic reference on
this material.</p>

<p>   The fundamental idea in optimal control is to formulate the goal of
control as the <em>long-term</em> optimization of a scalar cost function.
Let's introduce the basic concepts by considering a system that is even
simpler than the simple pendulum.</p>

<div data-type="example"><h1>Optimal Control Formulations for the Double Integrator</h1>

<p>
Consider the double integrator system $$\ddot{q} = u, \quad |u| \le 1.$$   If you would like a mechanical analog of the
system (I always do), then you can think about this as a unit mass
brick moving along the x-axis on a frictionless surface, with a control
input which provides a horizontal force, $u$.
The task is to design a
control system, $u = \pi(\bx,t)$, $\bx=[q,\dot{q}]^T$ to regulate this
brick to $\bx = [0,0]^T$.
<figure>
<img width="70%" src="figures/double_integrator_brick.svg"/>
<figcaption>The double integrator as a unit-mass brick on a frictionless surface</figcaption>
</figure>
</p>

<p>
In order to formulate this control design problem using optimal
control, we must define a scalar objective which scores the
long-term performance of running each candidate control policy,
$\pi(\bx,t)$, from each initial condition, $(\bx_0,t_0)$, and a list
of constraints that must be satisfied.   For the
task of driving the double integrator to the origin, one could imagine
a number of optimal control formulations which would accomplish the
task, e.g.:
<ul>
<li> Minimum time:  $\min_\pi t_f,$ subject to $\bx(t_0) =
  \bx_0,$ $\bx(t_f) = {\bf 0}.$ </li>
<li> Quadratic cost:  $\min_\pi \int_{t_0}^{\infty} \left[ \bx^T(t)
    {\bf Q} \bx(t) \right] dt,$ ${\bf Q}\succ0$.</li>
</ul>
where the first is a constrained optimization formulation which
optimizes time, and the second accrues a penalty at every instance
according to the distance that the state is away from the origin (in a
metric space defined by the matrix ${\bf Q}$), and therefore
encourages trajectories that go more directly towards the goal,
possibly at the expense of requiring a longer time to reach the goal
(in fact it will result in an exponential approach to the goal, where
as the minimum-time formulation will arrive at the goal in finite time).
Note that both optimization problems are only
well defined if it is possible for the system to actually reach the
origin, otherwise the minimum-time problem cannot satisfy the terminal
constraint, and the integral in the quadratic cost would not converge
to a finite value as time approaches infinity (fortunately the double
integrator system is controllable, and therefore can be driven to the
goal in finite time).</p>

<p> Note that the input limits, $|u|\le1$ are also required to make this problem
well-posed; otherwise both optimizations would result in the optimal policy
using infinite control input to approach the goal infinitely fast.  Besides input limits,
another common approach to limiting the control effort is to add an additional
quadratic cost on the input (or "effort"), e.g.
$\int \left[ \bu^T(t) {\bf R} \bu(t) \right] dt,$   ${\bf R}\succ0$.
This could be added to either formulation above.  We will examine many of these formulations
in some details in the examples worked out at the end of this chapter.
</p>

</div> <!-- end example -->

<p>Optimal control has a long history in robotics.  For instance, there has been
a great deal of work on the minimum-time problem for pick-and-place robotic
manipulators, and the linear quadratic regulator (LQR) and linear quadratic
regulator with Gaussian noise (LQG) have become essential tools for any
practicing controls engineer.  With increasingly powerful computers and
algorithms, the popularity of numerical optimal control has grown at an
incredible pace over the last few years.</p>

<div data-type="example"><h1>The minimum time problem for the double integrator</h1>

<p>
For more intuition, let's do an informal derivation of the solution to
the minimum time problem for the double integrator with input constraints:
\begin{align*}
  \minimize_{\pi} \quad & t_f\\
  \subjto \quad & \bx(t_0) = \bx_0, \\
  & \bx(t_f) = {\bf 0}, \\
  & \ddot{q}(t) = u(t), \\
  & |u| \le 1.
\end{align*}
What behavior would you expect an optimal controller exhibit?</p>

<p>
Your intuition might tell you that the best thing that the brick can
do, to reach the goal in minimum time with limited control input, is
to accelerate maximally towards the goal until reaching a critical
point, then hitting the brakes in order to come to a stop exactly at
the goal.  This would be called a <em>bang-bang</em> control policy;
these are often optimal for systems with bounded input, and it is in
fact optimal for the double integrator, although we will not prove it
until we have developed more tools.  <!-- leave the proof to the pontryagin notes -->.</p>

<p>
Let's work out the details of this bang-bang policy.  First, we can
figure out the states from which, when the brakes are fully
applied, the system comes to rest precisely at the origin.  Let's
start with the case where $q(0) < 0$, and $\dot{q}(0)>0$, and "hitting
the brakes" implies that $u=-1$ .
Integrating the equations, we have \begin{gather*} \ddot{q}(t) = u =
  -1 \\\dot{q}(t) = \dot{q}(0) - t \\ q(t) = q(0) + \dot{q}(0) t -
  \frac{1}{2} t^2.  \end{gather*}
Substituting $t = \dot{q}(0) - \dot{q}$ into the solution give $\dot{q}$ reveals
that the system orbits are parabolic arcs:
\[ q = -\frac{1}{2} \dot{q}^2 + c, \] with $c = q(0) + \frac{1}{2}\dot{q}^2(0)$.
<figure>
  <img width="80%" src="figures/double_integrator_orbits.svg"/>
  <figcaption>Two solutions for the system with $u=-1$</figcaption>
</figure>
Similarly, the solutions for $u=1$ are $q = \frac{1}{2} \dot{q}^2 + c$.
</p>

<p> Perhaps the most important of these orbits are the ones that pass directly
through the origin ($c=0$). Following our initial logic, if the system is going
slower than this $\dot{q}$ for any $q$, then the optimal thing to do is to slam
on the accelerator ($u=-\text{sgn}(q)$).  If it's going faster than the
$\dot{q}$ that we've solved for, then still the best thing to do is to brake;
but inevitably the system will overshoot the origin and have to come back.  We
can summarize this policy with: \[ u = \begin{cases}  +1 & \text{if } (\dot{q} < 0 \text{ and } q \le \frac{1}{2} \dot{q}^2) \text{ or } (\dot{q}\ge 0 \text{ and } q < -\frac{1}{2} \dot{q}^2) \\ 0 & \text{if } q=0 \text{
and } \dot{q}=0 \\ -1 & \text{otherwise} \end{cases} \]
<!--This policy is cartooned in
Figure~\ref{fig:mintime_double_int}.  %Trajectories of the system
%executing this policy are also included - the fundamental
%characteristic is that the system is accelerated as quickly as
%possible toward the switching surface, then rides the switching
%surface in to the origin. -->
<figure>
<img width="80%" src="figures/mintime_double_int.svg"/>
<figcaption>Candidate optimal (bang-bang) policy for the minimum-time
  double integrator problem.</figcaption>
</figure>
and illustrate some of the optimal solution trajectories:
<figure>
<img width="80%" src="figures/double_integrator_mintime_orbits.svg"/>
</figure>
</p>




</div>

<section data-type="sect2"><h1>Additive cost</h1>

<p>
As we begin to develop theoretical and algorithmic tools for optimal control,
we will see that some formulations are much easier to deal with than others.
One important example is the dramatic simplification that can come from
formulating objective functions using <em>additive
  cost</em>, because they often yield recursive solutions.  In the additive cost formulation, the long-term "score"
for a trajectory can be written as $$\int_0^T g(x(t),u(t)) dt,$$ where
$g()$ is the instantaneous cost, and $T$ can be either a finite
real number or $\infty$.  We will call a problem specification with a finite $T$ a
"finite-horizon" problem, and $T=\infty$ an "infinite-horizon"
problem.  Problems and solutions for infinite-horizon problems tend to be more elegant, but
care is required to make sure that the integral converges for the
optimal controller (typically
by having a goal state/action pair that allows the robot to accrue zero-cost).</p>

<p> At first glance, our minimum-time problem formulation for the double
integrator does not look like an additive cost problem.  However, we can write
in as an additive cost problem using an infinite horizon and the instantaneous
cost  $$g(x,u) = \begin{cases} 0 & \text{if } x=0   \text{ and } u=0, \\ 1 &
\text{otherwise.} \end{cases}$$</p>

<p>
We will examine a number of approaches to solving optimal control
problems throughout the next few chapters.  For the remainder of this chapter, we will
focus on additive cost problems and their solution via <em>dynamic programming</em>.</p>


</section> <!-- control design as an optimization -->

<section data-type="sect1"><h1>Optimal Control as Graph Search</h1>

<p> For systems with continuous states and continuous actions, dynamic
programming is a set of theoretical ideas surrounding additive cost optimal
control problems. For systems with a finite, discrete set of states and a
finite, discrete set of actions, dynamic programming also represents a set of very
efficient numerical <em>algorithm</em> which can compute optimal feedback
controllers. Many of you will have learned it before as a tool for graph search.
</p>

<p>Imagine you have a directed graph with states (or nodes) $\{s_1,s_2,...\} \in
S$ and "actions" associated with edges labeled as $\{a_1,a_2,...\} \in A$, as in
the following trivial example: <figure><img width="70%"
src="figures/graph_search.svg"/><figcaption>A simple directed
graph.</figcaption></figure> Let us also assume that each edge has an associate
weight or cost, using $g(s,a)$ to denote the cost of being in state $s$ and
taking action $a$. Furthemore we will denote the transition "dynamics" using \[ s[n+1] =
f(s[n],a[n]). \]</p>

<p>There are many algorithms for finding (or approximating) the optimal path
from a start to a goal on directed graphs.  In dynamic programming, we can use
the <em>value iteration</em> algorithm to solving simultaneously for the optimal
<em>cost-to-go</em> from every node to the goal -- we'll use $J_i^*$ to denote
the optimal cost-to-go from state $i$.  We initialize an estimate $\hat{J}^*=0$
for all $i$, then proceed with an iterative algorithm which sets \[ \hat{J}^*_i =
\min_{a \in A} \left[ g(s_i,a) + \hat{J}^*_{f(s_i,a)} \right]. \] This algorithm is
guaranteed to converge to the optimal cost-to-go, $\hat{J}^* \rightarrow J^*$,
and in practice does so rapidly. Assuming the graph has a goal state with a zero-cost
self-transition, then this cost-to-go function represents the weighted shortest
distance to the goal. </p>

<p>
Let's experiment with dynamic programming in a discrete-state,
discrete-action, discrete-time problem to further improve our
intuition.</p>

<div data-type="example"><h1>Grid World</h1>

<p>Imagine a robot living in a grid (finite state) world.  Wants to get to the goal
location.  Possibly has to negotiate cells with obstacles.  Actions
are to move up, down, left, right, or do nothing.  <elib>Sutton98</elib></p>

<todo>insert value iteration animation for min time and quadratic cost</todo>

<p>You can experiment with the grid world example in <em>drake</em> using
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples'));
GridWorld.runValueIteration();
</code></pre>
I recommend trying to edit the cost function and obstacles.</p>

</div> <!-- end grid world -->

Graph approximation of a continuous state space.

Stochastic shortest path.

<div data-type="example"><h1>Dynamic Programming for the Double Integrator</h1>

<p>You can run value iteration for the double integrator (using barycentric interpolation to interpolate between nodes) in <em>drake</em> using:
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples'));
DoubleIntegrator.runValueIteration();
</code></pre>
Again, you can easily try different cost functions by editing the code yourself.</p>

</div>

<div data-type="example"><h1>Dynamic Programming for the Simple Pendulum</h1>

<p>You can run value iteration for the simple pendulum (using barycentric interpolation to interpolate between nodes) in <em>drake</em> using:
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Pendulum'));
runValueIteration();
</code></pre>
Again, you can easily try different cost functions by editing the code yourself.</p>

</div>

</section> <!-- end of graph search -->

<section data-type="sect1"><h1>Continuous Dynamic Programming</h1>

<p>(coming in the next lectures...)</p>

</section> <!-- end continuous dp -->

<!-- ***************  end of dynamic programming **************   -->


<section data-type="chapter" class="chapter"><h1>Analytical Optimal Control</h1>

</section>

<!-- ***************  end of analytical optimal control **************   -->

<section data-type="chapter" class="chapter"><h1>Trajectory Optimization</h1>

</section>

<!-- ***************  end of trajectory optimization **************   -->

<section data-type="chapter" class="chapter"><h1>Feasible Motion Planning</h1>

</section>

<!-- ***************  end of feasible motion planning **************   -->

<section data-type="chapter" class="chapter"><h1>Verification</h1>

</section>

<!-- ***************  end of verification **************   -->

<section data-type="chapter" class="chapter"><h1>Robust Control, Stochastic Control, and Planning Under Uncertainty</h1>

</section>

<!-- ***************  end of robust control **************   -->

<section data-type="chapter" class="chapter"><h1>Feedback Motion Planning</h1>

</section>

<!-- ***************  end of feedback motion planning **************   -->

<!-- START OF PART III - Estimation and Learning -->


<appendix>

<section data-type="chapter" class="chapter" id="ch:manipulator"><h1>Robot Dynamics</h1>

<section data-type="sect1"><h1>Deriving the equations of motion (an example)</h1>

<p>The equations of motion for a standard robot can be derived using the method
of Lagrange.  Using $T$ as the total kinetic energy of the system, and $U$ as
the total potential energy of the system, $L = T-U$, and $Q_i$ as the
generalized force corresponding to $q_i$, the Lagrangian dynamic equations are:
$$\frac{d}{dt}\pd{L}{\dot{q}_i} - \pd{L}{q_i} = Q_i.$$ If you are not
comfortable with these equations, then any good book chapter on rigid body
mechanics can bring you up to speed -- try <elib>Craig89</elib> for a very
practical guide to robot kinematics/dynamics, <elib>Goldstein02</elib> for a
more hard-core dynamics text or <elib>Thornton03</elib> for a classical dynamics
text which is a nice read -- for now you can take them as a handle that you can
crank to generate equations of motion. </p>

<div data-type="example"><h1>Simple Double Pendulum</h1>

<figure>
  <img style="width:250px;" src="figures/simple_double_pend.svg"/>
  <figcaption>Simple double pendulum</figcaption>
</figure>

<p>
Consider the simple double pendulum with torque actuation at both joints and all
of the mass concentrated in two points (for simplicity).  Using $\bq =
[\theta_1,\theta_2]^T$, and ${\bf p}_1,{\bf p}_2$ to denote the locations of
$m_1,m_2$, respectively, the kinematics of this system are:

\begin{eqnarray*}
{\bf p}_1 =& l_1\begin{bmatrix} s_1 \\ - c_1 \end{bmatrix}, &{\bf p}_2  =
{\bf p}_1 + l_2\begin{bmatrix} s_{1+2} \\ - c_{1+2} \end{bmatrix} \\
\dot{{\bf p}}_1 =& l_1 \dot{q}_1\begin{bmatrix} c_1 \\ s_1 \end{bmatrix},
&\dot{{\bf p}}_2 = \dot{{\bf p}}_1 + l_2 (\dot{q}_1+\dot{q}_2) \begin{bmatrix} c_{1+2} \\ s_{1+2} \end{bmatrix}
\end{eqnarray*}

Note that $s_1$ is shorthand for $\sin(q_1)$, $c_{1+2}$ is shorthand for
$\cos(q_1+q_2)$, etc. From this we can easily write the kinetic and potential
energy:

\begin{align*}
T =& \frac{1}{2} m_1 \dot{\bf p}_1^T \dot{\bf p}_1 + \frac{1}{2} m_2
\dot{\bf p}_2^T \dot{\bf p}_2 \\
=& \frac{1}{2}(m_1 + m_2) l_1^2 \dot{q}_1^2 + \frac{1}{2} m_2 l_2^2 (\dot{q}_1 + \dot{q}_2)^2 + m_2 l_1 l_2 \dot{q}_1 (\dot{q}_1 + \dot{q}_2) c_2 \\
U =& m_1 g y_1 + m_2 g y_2 = -(m_1+m_2) g l_1 c_1 - m_2 g l_2 c_{1+2}
\end{align*}

Taking the partial derivatives $\pd{T}{q_i}$, $\pd{T}{\dot{q}_i}$, and
$\pd{U}{q_i}$ ($\pd{U}{\dot{q}_i}$ terms are always zero), then
$\frac{d}{dt}\pd{T}{\dot{q}_i}$, and plugging them into the Lagrangian, reveals
the equations of motion:

\begin{align*}
(m_1 + m_2) l_1^2 \ddot{q}_1& + m_2 l_2^2 (\ddot{q}_1 + \ddot{q}_2) + m_2 l_1 l_2 (2 \ddot{q}_1 + \ddot{q}_2) c_2 \\
&- m_2 l_1 l_2 (2 \dot{q}_1 + \dot{q}_2) \dot{q}_2 s_2 + (m_1 + m_2) l_1 g s_1 + m_2 g l_2 s_{1+2} = \tau_1 \\
m_2 l_2^2 (\ddot{q}_1 + \ddot{q}_2)& + m_2 l_1 l_2 \ddot{q}_1 c_2 + m_2 l_1 l_2
\dot{q}_1^2 s_2 + m_2 g l_2 s_{1+2} = \tau_2
\end{align*}

As we saw in chapter 1, numerically integrating (and animating) these equations
in MATLAB produces the expected result. </p>

</div>

</section>

<section data-type="sect1"><h1>The Manipulator Equations</h1>

<p> If you crank through the Lagrangian dynamics for a few simple robotic
manipulators, you will begin to see a pattern emerge - the resulting equations
of motion all have a characteristic form.  For example, the kinetic energy of
your robot can always be written in the form: $$T = \frac{1}{2} \dot{\bq}^T {\bf
H}(\bq) \dot{\bq},$$ where ${\bf H}$ is the state-dependent inertial matrix.
This abstraction affords some insight into general manipulator dynamics - for
example we know that ${\bf H}$ is always positive definite, and
symmetric<elib>Asada86</elib>(p.107) and has a beautiful sparsity
pattern<elib>Featherstone05</elib> that we'll attempt to take advantage of in
our algorithms.</p>


<p>   Continuing our abstractions, we find that the equations of motion of a
general robotic manipulator (sans kinematic loops) take the form $${\bf
H}(\bq)\ddot{\bq} + {\bf C}(\bq,\dot{\bq})\dot{\bq} + {\bf G}(\bq) = {\bf
B}(\bq)\bu,$$ where $\bq$ is the state vector, ${\bf H}$ is the inertial matrix,
${\bf C}$ captures Coriolis forces, and ${\bf G}$ captures potentials (such as
gravity).  The matrix ${\bf B}$ maps control inputs $\bu$ into generalized
forces.</p>

<div data-type="example"><h1>Manipulator Equation form of the Simple Double Pendulum</h1>
The equations of motion from Example 1 can be written compactly as:
\begin{align*}
{\bf H}(\bq) =& \begin{bmatrix} (m_1 + m_2)l_1^2 + m_2 l_2^2 + 2 m_2 l_1l_2 c_2 & m_2 l_2^2 + m_2 l_1 l_2 c_2 \\ m_2 l_2^2 + m_2 l_1 l_2 c_2 & m_2 l_2^2 \end{bmatrix} \\
{\bf C}(\bq,\dot\bq) =& \begin{bmatrix} 0 & -m_2 l_1 l_2 (2\dot{q}_1 + \dot{q}_2)s_2 \\ m_2 l_1 l_2 \dot{q}_1 s_2 & 0 \end{bmatrix} \\
{\bf G}(\bq) =& g \begin{bmatrix} (m_1 + m_2) l_1 s_1 + m_2 l_2
  s_{1+2} \\ m_2 l_2 s_{1+2} \end{bmatrix} , \quad {\bf B}
= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\end{align*}
Note that this choice of the ${\bf C}$ matrix was not unique.
</div>

<p> The manipulator equations are very general, but they do define some
important characteristics.  For example, $\ddot{\bq}$ is (state-dependent)
linearly related to the control input, $\bu$.   This observation justifies the
form of the dynamics assumed in \ref{eq:f1_plus_f2}.</p> Note that we have
chosen to use the notation of second-order systems (with $\dot{\bq}$ and
$\ddot{\bq}$ appearing in the equations) throughout this book.  Although I
believe it provides more clarity, there is an important limitation to this
notation: it is impossible to describe 3D rotations in "minimal coordinates"
using this notation without introducing kinematic singularities (like the famous
"gimbal lock"). For instance, a common singularity-free choice for representing
a 3D rotation is the unit quaternion, described by 4 real values (plus a norm
constraint).  However we can still represent the rotational velocity without
singularities using just 3 real values.  This means that the length of our
velocity vector is no longer the same as the length of our position vector.  For
this reason, you will see that most of the software in Drake uses the more
general notation with $\bv$ to represent velocity, $\bq$ to represent positions,
and the manipulator equations are written as \[ {\bf H}(\bq) \dot{\bv} + {\bf
C}(\bq,\bv)\bv + {\bf G}(\bq) = {\bf B}(\bq) \bu, \] which is not necessarily a
second-order system.  See <elib>Duindam06</elib> for a nice discussion of this
topic.</p>

</section>

<section data-type="sect1"><h1>Recursive Rigid-Body Dynamics Algorithms</h1>

<p>
The equations of motions for our machines get complicated quickly.
Fortunately, for robots with a tree-link kinematic structure, there
are very efficient and natural recursive algorithms for generating
these equations of motion.  For a detailed reference on these methods,
see <elib>Featherstone07</elib>; some people prefer reading about the
Articulated Body Method in <elib>Mirtich96</elib>.  The software libraries
distributed with this text include a complete implementation; you can
also some matlab algorithms available on Roy Featherstone's
website, or try googling "Open Dynamics Engine" or "Simulation
Construction Set".
</p>


</section> <!-- end deriving -->

</section> <!-- end robot dynamics chapter -->

</appendix>


<div id="footer">
<hr>
<table style="width:100%;">
  <tr><td><em>Underactuated Robotics</em></td><td align="right">&copy; Russ Tedrake, 2014</td></tr>
</table>
</div>

</body>
</html>
